{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biolearn.data_library import DataLibrary\n",
    "import pandas as pd\n",
    "# https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE19711\n",
    "\n",
    "data = DataLibrary().get(\"GSE19711\").load()\n",
    "data.metadata, data.dnam\n",
    "\n",
    "#elastic for selection, boost for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((540, 27579),\n",
       " id         cg00000292  cg00002426  cg00003994  cg00005847  cg00006414  \\\n",
       " GSM491937    0.756367    0.797859    0.068605    0.131004    0.076355   \n",
       " GSM491938    0.834702    0.859538    0.067469    0.197486    0.096817   \n",
       " GSM491939    0.774165    0.769661    0.056937    0.140949    0.156980   \n",
       " GSM491940    0.799517    0.854328    0.063802    0.168209    0.086761   \n",
       " GSM491941    0.819867    0.853270    0.061275    0.137825    0.079826   \n",
       " ...               ...         ...         ...         ...         ...   \n",
       " GSM492472    0.850640    0.811812    0.046001    0.165387    0.074648   \n",
       " GSM492473    0.785949    0.863659    0.065620    0.149490    0.078410   \n",
       " GSM492474    0.827493    0.815332    0.055990    0.155754    0.091335   \n",
       " GSM492475    0.817919    0.860943    0.062512    0.129134    0.065681   \n",
       " GSM492476    0.555077    0.511268    0.210708    0.066549    0.056630   \n",
       " \n",
       " id         cg00007981  cg00008493  cg00008713  cg00009407  cg00010193  ...  \\\n",
       " GSM491937    0.031687    0.946473    0.030616    0.073854    0.594014  ...   \n",
       " GSM491938    0.027651    0.956317    0.036103    0.080879    0.568817  ...   \n",
       " GSM491939    0.027933    0.952377    0.038230    0.064909    0.575405  ...   \n",
       " GSM491940    0.029277    0.961985    0.037684    0.065395    0.588644  ...   \n",
       " GSM491941    0.037950    0.960325    0.034399    0.076923    0.582535  ...   \n",
       " ...               ...         ...         ...         ...         ...  ...   \n",
       " GSM492472    0.031145    0.952253    0.036039    0.093094    0.636976  ...   \n",
       " GSM492473    0.028611    0.958846    0.035945    0.081228    0.607439  ...   \n",
       " GSM492474    0.042109    0.954158    0.040996    0.084361    0.607488  ...   \n",
       " GSM492475    0.025430    0.966155    0.045333    0.075592    0.611718  ...   \n",
       " GSM492476    0.176972    0.931410    0.015440    0.047197    0.636726  ...   \n",
       " \n",
       " id         cg27653134  cg27654142  cg27655855  cg27655905  cg27657249  \\\n",
       " GSM491937    0.746691    0.044225    0.818353    0.088685    0.161284   \n",
       " GSM491938    0.836914    0.193395    0.851130    0.074006    0.236641   \n",
       " GSM491939    0.755609    0.075082    0.822084    0.073700    0.187616   \n",
       " GSM491940    0.759248    0.073761    0.816040    0.076892    0.167379   \n",
       " GSM491941    0.807623    0.070806    0.436428    0.081855    0.163969   \n",
       " ...               ...         ...         ...         ...         ...   \n",
       " GSM492472    0.815250    0.066265    0.830900    0.090231    0.142626   \n",
       " GSM492473    0.850967    0.059076    0.827640    0.080969    0.172553   \n",
       " GSM492474    0.817877    0.086786    0.828041    0.092011    0.162151   \n",
       " GSM492475    0.788580    0.090769    0.839257    0.098245    0.169964   \n",
       " GSM492476    0.684253    0.094438    0.805744    0.147458    0.201412   \n",
       " \n",
       " id         cg27657283  cg27661264  cg27662379  cg27662877  cg27665659  \n",
       " GSM491937    0.054350    0.253198    0.028654    0.045494    0.046754  \n",
       " GSM491938    0.060951    0.427664    0.022261    0.044785    0.051954  \n",
       " GSM491939    0.054692    0.210370    0.027410    0.044007    0.046282  \n",
       " GSM491940    0.051071    0.241700    0.025137    0.032319    0.043402  \n",
       " GSM491941    0.046700    0.147942    0.026827    0.040882    0.041331  \n",
       " ...               ...         ...         ...         ...         ...  \n",
       " GSM492472    0.060186    0.371981    0.030636    0.045670    0.048369  \n",
       " GSM492473    0.044991    0.433100    0.034338    0.033494    0.037386  \n",
       " GSM492474    0.056461    0.422606    0.035688    0.035118    0.043518  \n",
       " GSM492475    0.047952    0.433005    0.034179    0.040766    0.048207  \n",
       " GSM492476    0.187679    0.499545    0.022769    0.092554    0.040923  \n",
       " \n",
       " [540 rows x 27578 columns],\n",
       "             age\n",
       " id             \n",
       " GSM491937  68.0\n",
       " GSM491938  81.0\n",
       " GSM491939  56.0\n",
       " GSM491940  62.0\n",
       " GSM491941  72.0\n",
       " ...         ...\n",
       " GSM492472  58.0\n",
       " GSM492473  61.0\n",
       " GSM492474  72.0\n",
       " GSM492475  65.0\n",
       " GSM492476  64.0\n",
       " \n",
       " [540 rows x 1 columns])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = data.dnam.transpose()\n",
    "X_df = pd.DataFrame(X)\n",
    "y = data.metadata['age']\n",
    "y = pd.DataFrame(y)\n",
    "cb= pd.concat([X_df, y], axis=1)\n",
    "cb.shape, X_df, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "# Assuming X_genomic is your genomic dataset and y_age is the corresponding age labels\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize pipeline with imputer and scaler\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Use mean imputation for missing values\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Fit and transform training data\n",
    "X_train_scaled = pipeline.fit_transform(X_train)\n",
    "\n",
    "# Transform test data\n",
    "X_test_scaled = pipeline.transform(X_test)\n",
    "\n",
    "# Initialize and fit the ElasticNetCV model\n",
    "alpha_values = np.logspace(-3, 3, 10)  # Range of alpha values to search\n",
    "l1_ratio_values = [0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 1]  # Range of l1_ratio values to search\n",
    "elasticnet_cv = ElasticNetCV(alphas=alpha_values, l1_ratio=l1_ratio_values, cv=5)\n",
    "elasticnet_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the selected alpha and l1_ratio values\n",
    "print(\"Selected alpha:\", elasticnet_cv.alpha_)\n",
    "print(\"Selected l1_ratio:\", elasticnet_cv.l1_ratio_)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_score = elasticnet_cv.score(X_test_scaled, y_test)\n",
    "print(\"Test Set R^2 Score:\", test_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-zero coefficients: 93\n",
      "Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 11, 12, 13, 15, 16, 17, 18, 19,\n",
      "       20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37,\n",
      "       38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 50, 51, 52, 53, 54, 55, 56,\n",
      "       57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 71, 72, 73, 75, 76,\n",
      "       77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 88, 90, 91, 92, 93, 94, 95, 96,\n",
      "       97, 98, 99],\n",
      "      dtype='int64')\n",
      "Column names of non-zero coefficients: Index(['Feature_0', 'Feature_1', 'Feature_2', 'Feature_3', 'Feature_4',\n",
      "       'Feature_5', 'Feature_6', 'Feature_7', 'Feature_8', 'Feature_9',\n",
      "       'Feature_11', 'Feature_12', 'Feature_13', 'Feature_15', 'Feature_16',\n",
      "       'Feature_17', 'Feature_18', 'Feature_19', 'Feature_20', 'Feature_21',\n",
      "       'Feature_22', 'Feature_23', 'Feature_24', 'Feature_25', 'Feature_26',\n",
      "       'Feature_27', 'Feature_28', 'Feature_29', 'Feature_30', 'Feature_31',\n",
      "       'Feature_32', 'Feature_33', 'Feature_34', 'Feature_35', 'Feature_36',\n",
      "       'Feature_37', 'Feature_38', 'Feature_39', 'Feature_40', 'Feature_41',\n",
      "       'Feature_42', 'Feature_43', 'Feature_44', 'Feature_45', 'Feature_46',\n",
      "       'Feature_47', 'Feature_48', 'Feature_50', 'Feature_51', 'Feature_52',\n",
      "       'Feature_53', 'Feature_54', 'Feature_55', 'Feature_56', 'Feature_57',\n",
      "       'Feature_58', 'Feature_59', 'Feature_60', 'Feature_61', 'Feature_62',\n",
      "       'Feature_63', 'Feature_64', 'Feature_65', 'Feature_66', 'Feature_67',\n",
      "       'Feature_68', 'Feature_69', 'Feature_71', 'Feature_72', 'Feature_73',\n",
      "       'Feature_75', 'Feature_76', 'Feature_77', 'Feature_78', 'Feature_79',\n",
      "       'Feature_80', 'Feature_81', 'Feature_82', 'Feature_83', 'Feature_84',\n",
      "       'Feature_85', 'Feature_86', 'Feature_88', 'Feature_90', 'Feature_91',\n",
      "       'Feature_92', 'Feature_93', 'Feature_94', 'Feature_95', 'Feature_96',\n",
      "       'Feature_97', 'Feature_98', 'Feature_99'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#####  feature selection\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.datasets import make_regression\n",
    "X_df, y = make_regression(n_samples=100, n_features=100, noise=0.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y, test_size=0.1, random_state=42)\n",
    "model = ElasticNet(alpha=0.4, l1_ratio=0.6) \n",
    "model.fit(X_train, y_train)\n",
    "coefficients = model.coef_\n",
    "\n",
    "# Print the number of non-zero coefficients\n",
    "num_nonzero = np.count_nonzero(coefficients)\n",
    "print(\"Number of non-zero coefficients:\", num_nonzero)\n",
    "\n",
    "non_zero_columns = pd.DataFrame(X_train).columns[coefficients != 0]\n",
    "print(non_zero_columns)\n",
    "X_train_df = pd.DataFrame(X_train, columns=[f\"Feature_{i}\" for i in range(X_train.shape[1])])\n",
    "\n",
    "non_zero_columns = X_train_df.columns[model.coef_ != 0]\n",
    "print(\"Column names of non-zero coefficients:\", non_zero_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cg00000292: 0.6474\n",
      "cg00003994: 4.7556\n",
      "cg00005847: 5.8235\n",
      "cg00006414: 30.5445\n",
      "cg00007981: 28.6039\n",
      "cg00008493: -0.3353\n",
      "cg00008713: 7.7542\n",
      "cg00009407: 2.8503\n",
      "cg00011459: -0.0000\n",
      "cg00012199: 1.9435\n",
      "cg00012386: 2.2607\n",
      "cg00012792: 1.8734\n",
      "cg00013618: 0.0000\n",
      "cg00014085: -3.9383\n",
      "cg00014837: 1.5995\n",
      "cg00015770: 1.7930\n",
      "cg00016968: 4.2828\n",
      "cg00019495: -3.4246\n",
      "cg00020533: -1.5448\n",
      "cg00021527: 3.0546\n",
      "cg00022606: 3.1943\n",
      "cg00022866: 2.3855\n",
      "cg00024396: 0.1302\n",
      "cg00025138: 4.1996\n",
      "cg00025991: -5.0169\n",
      "cg00027083: -5.3641\n",
      "cg00027674: 4.4294\n",
      "cg00029931: -3.1816\n",
      "cg00030047: -0.1563\n",
      "cg00031162: -0.0948\n",
      "cg00032227: -0.8227\n",
      "cg00032666: -0.1911\n",
      "cg00033773: 1.2034\n",
      "cg00035347: -1.8498\n",
      "cg00035623: 3.3269\n",
      "cg00037763: 3.3437\n",
      "cg00037940: -7.2966\n",
      "cg00040861: -3.4526\n",
      "cg00040873: 3.1898\n",
      "cg00041575: -1.9396\n",
      "cg00042156: -1.6376\n",
      "cg00043004: -2.0238\n",
      "cg00043080: -3.7378\n",
      "cg00044245: -0.0000\n",
      "cg00044729: 55.8326\n",
      "cg00047050: 2.4033\n",
      "cg00049986: -0.1999\n",
      "cg00050312: 41.4529\n",
      "cg00051623: -2.9348\n",
      "cg00053292: -2.4973\n",
      "cg00053647: 2.0630\n",
      "cg00054706: -1.9398\n",
      "cg00055233: -0.7486\n",
      "cg00056767: 4.1382\n",
      "cg00057593: -2.7520\n",
      "cg00058938: -3.4498\n",
      "cg00059225: -3.9812\n",
      "cg00059424: 4.1637\n",
      "cg00059740: 49.6691\n",
      "cg00059930: -2.9587\n",
      "cg00060762: 7.4454\n",
      "cg00060882: -3.4057\n",
      "cg00061059: -0.0000\n",
      "cg00061629: -5.4228\n",
      "cg00063144: -3.8712\n",
      "cg00065385: 0.0000\n",
      "cg00065408: -6.3083\n",
      "cg00066153: -0.3015\n",
      "cg00066816: 1.7047\n",
      "cg00067471: 17.3924\n",
      "cg00069261: -2.2457\n",
      "cg00071998: 0.2058\n",
      "cg00072216: 7.3289\n",
      "cg00075967: -2.4267\n",
      "cg00076645: -2.5878\n",
      "cg00077457: 0.0319\n",
      "cg00078194: -0.6493\n",
      "cg00078867: -0.0000\n",
      "cg00079056: -0.1716\n",
      "cg00080012: 5.9236\n",
      "cg00081935: -4.1874\n",
      "cg00081975: 1.3999\n",
      "cg00083720: -0.2905\n",
      "cg00083937: 1.5713\n",
      "cg00084687: 5.1835\n",
      "cg00089071: -3.1377\n",
      "cg00090147: -3.2995\n"
     ]
    }
   ],
   "source": [
    "non_zero_coefficients = coefficients[non_zero_indices]\n",
    "non_zero_feature_names = X_df.columns[non_zero_indices]\n",
    "for coef, feature_name in zip(non_zero_coefficients, non_zero_feature_names):\n",
    "    print(f\"{feature_name}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_choose = X_df[[\"cg00000292\",\t\"cg00003994\",\t\"cg00005847\",\t\"cg00006414\",\t\"cg00007981\",\t\"cg00008493\",\t\"cg00008713\",\t\"cg00009407\",\t\"cg00011459\",\t\"cg00012199\",\t\"cg00012386\",\t\"cg00012792\",\t\"cg00013618\",\t\"cg00014085\",\t\"cg00014837\",\t\"cg00015770\",\t\"cg00016968\",\t\"cg00019495\",\t\"cg00020533\",\t\"cg00021527\",\t\"cg00022606\",\t\"cg00022866\",\t\"cg00024396\",\t\"cg00025138\",\t\"cg00025991\",\t\"cg00027083\",\t\"cg00027674\",\t\"cg00029931\",\t\"cg00030047\",\t\"cg00031162\",\t\"cg00032227\",\t\"cg00032666\",\t\"cg00033773\",\t\"cg00035347\",\t\"cg00035623\",\t\"cg00037763\",\t\"cg00037940\",\t\"cg00040861\",\t\"cg00040873\",\t\"cg00041575\",\t\"cg00042156\",\t\"cg00043004\",\t\"cg00043080\",\t\"cg00044245\",\t\"cg00044729\",\t\"cg00047050\",\t\"cg00049986\",\t\"cg00050312\",\t\"cg00051623\",\t\"cg00053292\",\t\"cg00053647\",\t\"cg00054706\",\t\"cg00055233\",\t\"cg00056767\",\t\"cg00057593\",\t\"cg00058938\",\t\"cg00059225\",\t\"cg00059424\",\t\"cg00059740\",\t\"cg00059930\",\t\"cg00060762\",\t\"cg00060882\",\t\"cg00061059\",\t\"cg00061629\",\t\"cg00063144\",\t\"cg00065385\",\t\"cg00065408\",\t\"cg00066153\",\t\"cg00066816\",\t\"cg00067471\",\t\"cg00069261\",\t\"cg00071998\",\t\"cg00072216\",\t\"cg00075967\",\t\"cg00076645\",\t\"cg00077457\",\t\"cg00078194\",\t\"cg00078867\",\t\"cg00079056\",\t\"cg00080012\",\t\"cg00081935\",\t\"cg00081975\",\t\"cg00083720\",\t\"cg00083937\",\t\"cg00084687\",\t\"cg00089071\",\t\"cg00090147\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost model train score: 0.9901515471112556\n",
      "XGBoost model test score: 0.8338680125606357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/petra/Desktop/study/UCSF/datasci 223 python/datasci_223/.conda/lib/python3.11/site-packages/xgboost/sklearn.py:889: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "/Users/petra/Desktop/study/UCSF/datasci 223 python/datasci_223/.conda/lib/python3.11/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "###### XGBoost\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "#X_choose, y = make_regression(n_samples=100, n_features=100, noise=0.1, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_choose, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train an XGBoost model\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=10000, learning_rate=0.15, max_depth=2, reg_alpha=0.7, reg_lambda=0.1)\n",
    "xgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], eval_metric='rmse', early_stopping_rounds=10, verbose=False)\n",
    "\n",
    "# Evaluate the model\n",
    "train_score = xgb_model.score(X_train, y_train)\n",
    "test_score = xgb_model.score(X_test, y_test)\n",
    "print(\"XGBoost model train score:\", train_score)\n",
    "print(\"XGBoost model test score:\", test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on training set: 197.88939012726706\n",
      "MSE on testing set: 2611.7418075311098\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Predictions on the training and testing sets\n",
    "y_train_pred = xgb_model.predict(X_train)\n",
    "y_test_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate the Mean Squared Error\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "print(\"MSE on training set:\", mse_train)\n",
    "print(\"MSE on testing set:\", mse_test)\n",
    "\n",
    "from sklearn.metrics import r2_score, roc_auc_score\n",
    "\n",
    "# Calculate R-squared (R2) score\n",
    "r2 = r2_score(y_test, y_test_pred)\n",
    "print(\"R-squared (R2) score:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "MSE on training set with best model: 2760.316786133939\n",
      "MSE on testing set with best model: 7296.453596243734\n"
     ]
    }
   ],
   "source": [
    "#########random forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],  # Number of trees in the forest\n",
    "    'max_depth': [None, 10, 20],      # Maximum depth of the trees\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4]     # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# Instantiate the Random Forest regressor\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best estimator\n",
    "best_params = grid_search.best_params_\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"Best parameters:\", best_params)\n",
    "\n",
    "# Evaluate the best model\n",
    "y_train_pred = best_rf_model.predict(X_train)\n",
    "y_test_pred = best_rf_model.predict(X_test)\n",
    "\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "print(\"MSE on training set with best model:\", mse_train)\n",
    "print(\"MSE on testing set with best model:\", mse_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on testing set with best model: 6544.6839326361205\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Instantiate a new Random Forest model with the best parameters\n",
    "best_rf_model = RandomForestRegressor(n_estimators=1000,\n",
    "                                       max_depth=5,\n",
    "                                       min_samples_split=2,\n",
    "                                       min_samples_leaf=4,\n",
    "                                       random_state=10)\n",
    "\n",
    "# Fit the model to the training data\n",
    "best_rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "y_test_pred = best_rf_model.predict(X_test)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "print(\"MSE on testing set with best model:\", mse_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 10, 'epsilon': 0.1, 'kernel': 'linear'}\n",
      "R-squared on training set with best model: 0.9999995015487311\n",
      "R-squared on testing set with best model: 0.7136437588710891\n",
      "MAE on training set with best model: 0.10007764714881104\n",
      "MAE on testing set with best model: 59.23761991852349\n",
      "MSE on training set with best model: 0.010015605367927432\n",
      "MSE on testing set with best model: 4501.77342925488\n"
     ]
    }
   ],
   "source": [
    "####### SVM\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "# Instantiate the SVM regressor with the desired hyperparameters\n",
    "svm_model = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
    "\n",
    "# Fit the SVM model to the training data\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'C': [0.1, 1, 10],\n",
    "    'epsilon': [0.1, 0.2, 0.5]\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best estimator\n",
    "best_params = grid_search.best_params_\n",
    "best_svm_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"Best parameters:\", best_params)\n",
    "\n",
    "# Evaluate the best model\n",
    "y_train_pred = best_svm_model.predict(X_train)\n",
    "y_test_pred = best_svm_model.predict(X_test)\n",
    "\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "print(\"R-squared on training set with best model:\", train_r2)\n",
    "print(\"R-squared on testing set with best model:\", test_r2)\n",
    "print(\"MAE on training set with best model:\", train_mae)\n",
    "print(\"MAE on testing set with best model:\", test_mae)\n",
    "print(\"MSE on training set with best model:\", train_mse)\n",
    "print(\"MSE on testing set with best model:\", test_mse)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
